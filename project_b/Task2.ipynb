{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WYMfvCNPwpm"
   },
   "source": [
    "# Project B: Knowledge Distillation for Building Lightweight Deep Learning Models in Visual Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vA8ppgB2P0aJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTRAIN_BATCHES = 4\\nTEST_BATCHES = 4\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from typing import Union\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "# Not originally included\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "NUM_INIT_EPOCHS = 10\n",
    "NUM_FINE_EPOCHS = 25\n",
    "\n",
    "ENTROPY_ZERO_FILLER = 1e-15\n",
    "NUM_CLASSES = 2 #SSA or HP\n",
    "TRAIN_BATCHES = int(np.floor(2176/BATCH_SIZE))\n",
    "TEST_BATCHES = int(np.floor(976/BATCH_SIZE))\n",
    "\n",
    "# For quick testing purposes ONLY\n",
    "\"\"\"\n",
    "TRAIN_BATCHES = 4\n",
    "TEST_BATCHES = 4\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2EFLQROP2R7"
   },
   "source": [
    "## Data loading/augmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2176 images belonging to 2 classes.\n",
      "Found 2176 images belonging to 2 classes.\n",
      "Found 976 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Adapted from Project A's HMT Dataset loading code to use the ImageDataGenerator to augment the data and preload labels/image batches\n",
    "\n",
    "img_dir = 'mhist_dataset/images'\n",
    "train_dir = 'mhist_dataset/images/train'\n",
    "test_dir = 'mhist_dataset/images/test'\n",
    "anno_csv = 'mhist_dataset/annotations.csv'\n",
    "\n",
    "\n",
    "if not os.path.isdir(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "if not os.path.isdir(test_dir):\n",
    "    os.mkdir(test_dir)\n",
    "    \n",
    "if not os.path.isdir(os.path.join(train_dir, '01_HP')):\n",
    "    os.mkdir(os.path.join(train_dir, '01_HP'))\n",
    "if not os.path.isdir(os.path.join(train_dir, '02_SSA')):\n",
    "    os.mkdir(os.path.join(train_dir, '02_SSA'))\n",
    "if not os.path.isdir(os.path.join(test_dir, '01_HP')):\n",
    "    os.mkdir(os.path.join(test_dir, '01_HP'))\n",
    "if not os.path.isdir(os.path.join(test_dir, '02_SSA')):\n",
    "    os.mkdir(os.path.join(test_dir, '02_SSA'))\n",
    "    \n",
    "# load csv\n",
    "# label struct: [HP, SSA]\n",
    "# labels as a list are sorted in alphabetical order as per the csv\n",
    "\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "train_img = []\n",
    "test_img = []\n",
    "\n",
    "with open(anno_csv, 'r') as csvfile:\n",
    "    first_row = True\n",
    "    for row in csv.reader(csvfile):\n",
    "        if first_row:\n",
    "            first_row = False\n",
    "            continue\n",
    "        if row[3] == 'train':\n",
    "            train_img.append(row[0])\n",
    "            if row[1] == 'HP':\n",
    "                train_labels.append([1, 0])\n",
    "            elif row[1] == 'SSA':\n",
    "                train_labels.append([0, 1])\n",
    "        elif row[3] == 'test':\n",
    "            test_img.append(row[0])\n",
    "            if row[1] == 'HP':\n",
    "                test_labels.append([1, 0])\n",
    "            elif row[1] == 'SSA':\n",
    "                test_labels.append([0, 1])\n",
    "        if row[0] in os.listdir(img_dir):\n",
    "            if row[1] == 'HP' and row[3] == 'train':\n",
    "                os.rename(os.path.join(img_dir, row[0]), os.path.join(train_dir, '01_HP', row[0]))\n",
    "            elif row[1] == 'SSA' and row[3] == 'train':\n",
    "                os.rename(os.path.join(img_dir, row[0]), os.path.join(train_dir, '02_SSA', row[0]))\n",
    "            elif row[1] == 'HP' and row[3] == 'test':\n",
    "                os.rename(os.path.join(img_dir, row[0]), os.path.join(test_dir, '01_HP', row[0]))\n",
    "            elif row[1] == 'SSA' and row[3] == 'test':\n",
    "                os.rename(os.path.join(img_dir, row[0]), os.path.join(test_dir, '02_SSA', row[0]))\n",
    "    \n",
    "# Data Augmentation using ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.,\n",
    "shear_range=0.1,\n",
    "rotation_range=15,\n",
    "horizontal_flip=True,\n",
    "vertical_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "class_mode='categorical',\n",
    "interpolation='bilinear',\n",
    "target_size=(224, 224),\n",
    "batch_size=BATCH_SIZE,\n",
    "shuffle=True) # 68 batches of 32\n",
    "\n",
    "t2_generator = test_datagen.flow_from_directory(train_dir,\n",
    "class_mode='categorical',\n",
    "interpolation='bilinear',\n",
    "target_size=(224, 224),\n",
    "batch_size=BATCH_SIZE,\n",
    "shuffle=True) # 68 batches of 32\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "class_mode='categorical',\n",
    "interpolation='bilinear',\n",
    "target_size=(224, 224),\n",
    "batch_size=BATCH_SIZE,\n",
    "shuffle=False) # 30.5 batches of 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAZwfvW5P63q",
    "tags": []
   },
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zINgDkA7P7BP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50v2 (Functional)     (None, 1000)              25613800  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 2002      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,615,802\n",
      "Trainable params: 25,570,362\n",
      "Non-trainable params: 45,440\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Funct  (None, 1000)             3538984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 2002      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,540,986\n",
      "Trainable params: 3,506,874\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Funct  (None, 1000)             3538984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 2002      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,540,986\n",
      "Trainable params: 3,506,874\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "# citing https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/ResNet50V2, https://www.kaggle.com/code/suniliitb96/tutorial-keras-transfer-learning-with-resnet50/notebook,\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2/MobileNetV2 and https://github.com/Abhi-T/MNIST-CLASSIFIER-From-Scratch/blob/main/MNIST__handwritten_digit_Model.ipynb\n",
    "\n",
    "# Build CNN teacher.\n",
    "\n",
    "teacher_model = tf.keras.Sequential()\n",
    "\n",
    "# your code start from here for step 2\n",
    "\n",
    "teacher_model.add(tf.keras.applications.resnet_v2.ResNet50V2(classifier_activation = None, input_shape = (224, 224, 3)))\n",
    "teacher_model.add(tf.keras.layers.Dense(NUM_CLASSES)) \n",
    "print(teacher_model.summary())\n",
    "\n",
    "# Build fully connected students\n",
    "student_kd_model = tf.keras.Sequential()\n",
    "student_kd_model.add(tf.keras.applications.mobilenet_v2.MobileNetV2(classifier_activation = None, input_shape = (224, 224, 3)))\n",
    "student_kd_model.add(tf.keras.layers.Dense(NUM_CLASSES)) \n",
    "print(student_kd_model.summary())\n",
    "\n",
    "student_scratch_model = tf.keras.Sequential()\n",
    "student_scratch_model.add(tf.keras.applications.mobilenet_v2.MobileNetV2(classifier_activation = None, input_shape = (224, 224, 3)))\n",
    "student_scratch_model.add(tf.keras.layers.Dense(NUM_CLASSES)) \n",
    "print(student_scratch_model.summary())\n",
    "\n",
    "\n",
    "# your code start from here for step 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JWGucyrQGav",
    "tags": []
   },
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DhzBP6ZLQJ57"
   },
   "outputs": [],
   "source": [
    "def compute_teacher_loss(model, images, labels, **kwargs):\n",
    "    \"\"\"Compute class knowledge distillation teacher loss for given images\n",
    "     and labels.\n",
    "\n",
    "    Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "    kwargs: n/a\n",
    "\n",
    "    Returns:\n",
    "    Scalar loss Tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    class_logits = model(images, training=True)\n",
    "\n",
    "    # Compute cross-entropy loss for classes.\n",
    "    \n",
    "    cross_entropy_loss_value = tf.keras.losses.categorical_crossentropy(labels, tf.nn.softmax(class_logits))\n",
    "\n",
    "    return cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS8xkuH0QbOS",
    "tags": []
   },
   "source": [
    "## Student (KD) Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lDKia4gPQMIr"
   },
   "outputs": [],
   "source": [
    "# adapted from https://keras.io/examples/vision/knowledge_distillation/\n",
    "\n",
    "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
    "                      temperature: Union[float, tf.Tensor]):\n",
    "    \"\"\"Compute distillation loss.\n",
    "\n",
    "    This function computes cross entropy between softened logits and softened\n",
    "    targets. The resulting loss is scaled by the squared temperature so that\n",
    "    the gradient magnitude remains approximately constant as the temperature is\n",
    "    changed. For reference, see Hinton et al., 2014, \"Distilling the knowledge in\n",
    "    a neural network.\"\n",
    "\n",
    "    Args:\n",
    "    teacher_logits: A Tensor of logits provided by the teacher.\n",
    "    student_logits: A Tensor of logits provided by the student, of the same\n",
    "      shape as `teacher_logits`.\n",
    "    temperature: Temperature to use for distillation.\n",
    "\n",
    "    Returns:\n",
    "    A scalar Tensor containing the distillation loss.\n",
    "    \"\"\"\n",
    "    # your code start from here for step 3\n",
    "    soft_targets = teacher_logits / temperature\n",
    "\n",
    "    return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
    "\n",
    "def compute_student_loss(student_model, images, labels, **kwargs):\n",
    "    \"\"\"Compute class knowledge distillation student loss for given images\n",
    "     and labels.\n",
    "\n",
    "    Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "    kwargs:\n",
    "        teacher_model: Teacher model\n",
    "        temperature: Temperature hyperparameter\n",
    "        alpha: Alpha hyperparameter\n",
    "\n",
    "    Returns:\n",
    "    Scalar loss Tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    teacher_model = kwargs['teacher_model']\n",
    "    temperature = kwargs['temperature']\n",
    "    alpha = kwargs['alpha']\n",
    "    \n",
    "    student_class_logits = student_model(images, training=True)\n",
    "\n",
    "    # Compute class distillation loss between student class logits and\n",
    "    # softened teacher class targets probabilities.\n",
    "\n",
    "    teacher_class_logits = teacher_model(images, training=False)\n",
    "    distillation_loss_value = distillation_loss(teacher_class_logits, student_class_logits, temperature)\n",
    "\n",
    "    # Compute cross-entropy loss with hard targets.\n",
    "    \n",
    "    cross_entropy_loss_value = tf.keras.losses.categorical_crossentropy(labels, tf.nn.softmax(student_class_logits))\n",
    "\n",
    "    total_loss = alpha * cross_entropy_loss_value + (1 - alpha) * distillation_loss_value\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student (Scratch) Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_student_scratch_loss(model, images, labels, **kwargs):\n",
    "    \"\"\"Compute class student (scratch) loss for given images\n",
    "     and labels.\n",
    "\n",
    "    Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "    kwargs:\n",
    "        temperature: Temperature hyperparameter\n",
    "\n",
    "    Returns:\n",
    "    Scalar loss Tensor.\n",
    "    \"\"\"\n",
    "    temperature = kwargs['temperature']\n",
    "    \n",
    "    class_logits = model(images, training=True)\n",
    "\n",
    "    # Compute cross-entropy loss for classes.\n",
    "    \n",
    "    cross_entropy_loss_value = tf.keras.losses.categorical_crossentropy(labels, tf.nn.softmax(class_logits/temperature)) * temperature ** 2\n",
    "\n",
    "    return cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJ1uyvurQ3w4",
    "tags": []
   },
   "source": [
    "# Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EtoLbp8uQ4Vl"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_num_correct(model, images, labels):\n",
    "    \"\"\"Compute number of correctly classified images in a batch.\n",
    "\n",
    "    Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "    Returns:\n",
    "    Number of correctly classified images.\n",
    "    \"\"\"\n",
    "    class_logits = model(images, training=False)\n",
    "    return tf.reduce_sum(\n",
    "        tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
    "              tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, compute_loss_fn, num_epochs, learning_rate, **kwargs):\n",
    "    \"\"\"Perform training and evaluation for a given model.\n",
    "\n",
    "    Args:\n",
    "    model: Main Instance of tf.keras.Model.\n",
    "    compute_loss_fn: A function that computes the training loss given the\n",
    "        images, and labels.\n",
    "    num_epochs: Number of epochs to train for\n",
    "    learning_rate: Optimizer learning rate\n",
    "    kwargs: Passed through to loss fn\n",
    "    \"\"\"\n",
    "\n",
    "    # your code start from here for step 4\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    train_generator.reset()\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Run training.\n",
    "        print('Epoch {}: '.format(epoch), end='')\n",
    "\n",
    "        #for images, labels in mhist_train:\n",
    "        for batch in range(TRAIN_BATCHES):\n",
    "            \n",
    "            images, labels = train_generator.next()\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                loss_value = compute_loss_fn(model, images, labels, **kwargs)\n",
    "\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Run evaluation.\n",
    "        num_correct = 0\n",
    "        num_total = BATCH_SIZE*TEST_BATCHES\n",
    "        \n",
    "        test_generator.reset()\n",
    "        \n",
    "        for batch in range(TEST_BATCHES):\n",
    "            images, labels = test_generator.next()\n",
    "            num_correct += compute_num_correct(model,images,labels)[0]\n",
    "        accuracy = num_correct / num_total * 100\n",
    "        print(\"Class_accuracy: \" + '{:.2f}%'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQL1lJdaRPT1"
   },
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-AGHbyABRPz3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Run: Initial\n",
      "Epoch 1: Class_accuracy: 77.46%\n",
      "Epoch 2: Class_accuracy: 62.70%\n",
      "Epoch 3: Class_accuracy: 68.44%\n",
      "Epoch 4: Class_accuracy: 76.33%\n",
      "Epoch 5: Class_accuracy: 60.66%\n",
      "Epoch 6: Class_accuracy: 74.18%\n",
      "Epoch 7: Class_accuracy: 75.10%\n",
      "Epoch 8: Class_accuracy: 78.07%\n",
      "Epoch 9: Class_accuracy: 79.51%\n",
      "Epoch 10: Class_accuracy: 73.87%\n",
      "\n",
      "Teacher Run: Fine\n",
      "Epoch 1: Class_accuracy: 80.23%\n",
      "Epoch 2: Class_accuracy: 81.97%\n",
      "Epoch 3: Class_accuracy: 83.20%\n",
      "Epoch 4: Class_accuracy: 82.48%\n",
      "Epoch 5: Class_accuracy: 83.50%\n",
      "Epoch 6: Class_accuracy: 82.89%\n",
      "Epoch 7: Class_accuracy: 84.32%\n",
      "Epoch 8: Class_accuracy: 83.30%\n",
      "Epoch 9: Class_accuracy: 83.40%\n",
      "Epoch 10: Class_accuracy: 83.50%\n",
      "Epoch 11: Class_accuracy: 84.12%\n",
      "Epoch 12: Class_accuracy: 84.53%\n",
      "Epoch 13: Class_accuracy: 82.38%\n",
      "Epoch 14: Class_accuracy: 85.45%\n",
      "Epoch 15: Class_accuracy: 85.66%\n",
      "Epoch 16: Class_accuracy: 85.04%\n",
      "Epoch 17: Class_accuracy: 85.55%\n",
      "Epoch 18: Class_accuracy: 85.86%\n",
      "Epoch 19: Class_accuracy: 85.04%\n",
      "Epoch 20: Class_accuracy: 84.12%\n",
      "Epoch 21: Class_accuracy: 84.43%\n",
      "Epoch 22: Class_accuracy: 85.66%\n",
      "Epoch 23: Class_accuracy: 85.04%\n",
      "Epoch 24: Class_accuracy: 85.04%\n",
      "Epoch 25: Class_accuracy: 85.25%\n",
      "\n",
      "\n",
      "Student (KD) Run: Initial\n",
      "Epoch 1: Class_accuracy: 63.22%\n",
      "Epoch 2: Class_accuracy: 63.22%\n",
      "Epoch 3: Class_accuracy: 63.22%\n",
      "Epoch 4: Class_accuracy: 63.22%\n",
      "Epoch 5: Class_accuracy: 63.22%\n",
      "Epoch 6: Class_accuracy: 63.22%\n",
      "Epoch 7: Class_accuracy: 63.22%\n",
      "Epoch 8: Class_accuracy: 63.22%\n",
      "Epoch 9: Class_accuracy: 63.22%\n",
      "Epoch 10: Class_accuracy: 63.22%\n",
      "\n",
      "Student (KD) Run: Fine\n",
      "Epoch 1: Class_accuracy: 63.22%\n",
      "Epoch 2: Class_accuracy: 63.22%\n",
      "Epoch 3: Class_accuracy: 36.78%\n",
      "Epoch 4: Class_accuracy: 63.22%\n",
      "Epoch 5: Class_accuracy: 63.22%\n",
      "Epoch 6: Class_accuracy: 36.78%\n",
      "Epoch 7: Class_accuracy: 36.78%\n",
      "Epoch 8: Class_accuracy: 63.22%\n",
      "Epoch 9: Class_accuracy: 36.78%\n",
      "Epoch 10: Class_accuracy: 63.22%\n",
      "Epoch 11: Class_accuracy: 63.22%\n",
      "Epoch 12: Class_accuracy: 36.78%\n",
      "Epoch 13: Class_accuracy: 63.22%\n",
      "Epoch 14: Class_accuracy: 63.22%\n",
      "Epoch 15: Class_accuracy: 63.22%\n",
      "Epoch 16: Class_accuracy: 63.22%\n",
      "Epoch 17: Class_accuracy: 63.22%\n",
      "Epoch 18: Class_accuracy: 63.22%\n",
      "Epoch 19: Class_accuracy: 63.22%\n",
      "Epoch 20: Class_accuracy: 63.22%\n",
      "Epoch 21: Class_accuracy: 36.78%\n",
      "Epoch 22: Class_accuracy: 36.78%\n",
      "Epoch 23: Class_accuracy: 63.22%\n",
      "Epoch 24: Class_accuracy: 63.22%\n",
      "Epoch 25: Class_accuracy: 36.78%\n",
      "\n",
      "\n",
      "Student (Scratch) Run: Initial\n",
      "Epoch 1: Class_accuracy: 63.22%\n",
      "Epoch 2: Class_accuracy: 63.22%\n",
      "Epoch 3: Class_accuracy: 63.22%\n",
      "Epoch 4: Class_accuracy: 63.22%\n",
      "Epoch 5: Class_accuracy: 63.22%\n",
      "Epoch 6: Class_accuracy: 63.22%\n",
      "Epoch 7: Class_accuracy: 63.22%\n",
      "Epoch 8: Class_accuracy: 63.22%\n",
      "Epoch 9: Class_accuracy: 63.22%\n",
      "Epoch 10: Class_accuracy: 40.27%\n",
      "\n",
      "Student (Scratch) Run: Fine\n",
      "Epoch 1: Class_accuracy: 63.22%\n",
      "Epoch 2: Class_accuracy: 62.30%\n",
      "Epoch 3: Class_accuracy: 63.11%\n",
      "Epoch 4: Class_accuracy: 62.19%\n",
      "Epoch 5: Class_accuracy: 62.50%\n",
      "Epoch 6: Class_accuracy: 62.09%\n",
      "Epoch 7: Class_accuracy: 63.22%\n",
      "Epoch 8: Class_accuracy: 62.70%\n",
      "Epoch 9: Class_accuracy: 60.86%\n",
      "Epoch 10: Class_accuracy: 61.58%\n",
      "Epoch 11: Class_accuracy: 64.04%\n",
      "Epoch 12: Class_accuracy: 57.38%\n",
      "Epoch 13: Class_accuracy: 63.32%\n",
      "Epoch 14: Class_accuracy: 63.83%\n",
      "Epoch 15: Class_accuracy: 64.65%\n",
      "Epoch 16: Class_accuracy: 64.24%\n",
      "Epoch 17: Class_accuracy: 63.22%\n",
      "Epoch 18: Class_accuracy: 63.52%\n",
      "Epoch 19: Class_accuracy: 63.63%\n",
      "Epoch 20: Class_accuracy: 63.22%\n",
      "Epoch 21: Class_accuracy: 63.83%\n",
      "Epoch 22: Class_accuracy: 64.55%\n",
      "Epoch 23: Class_accuracy: 63.42%\n",
      "Epoch 24: Class_accuracy: 63.32%\n",
      "Epoch 25: Class_accuracy: 63.32%\n"
     ]
    }
   ],
   "source": [
    "# your code start from here for step 5 \n",
    "\n",
    "print(\"Teacher Run: Initial\")\n",
    "train_and_evaluate(teacher_model, compute_teacher_loss, NUM_INIT_EPOCHS, 1e-4)\n",
    "print(\"\\nTeacher Run: Fine\")\n",
    "teach_acc = train_and_evaluate(teacher_model, compute_teacher_loss, NUM_FINE_EPOCHS, 1e-5)\n",
    "\n",
    "print(\"\\n\\nStudent (KD) Run: Initial\")\n",
    "train_and_evaluate(student_kd_model, compute_student_loss, NUM_INIT_EPOCHS, 1e-3, teacher_model=teacher_model, temperature=4, alpha=0.5)\n",
    "print(\"\\nStudent (KD) Run: Fine\")\n",
    "skd_acc = train_and_evaluate(student_kd_model, compute_student_loss, NUM_FINE_EPOCHS, 1e-4, teacher_model=teacher_model, temperature=4, alpha=0.5)\n",
    "\n",
    "print(\"\\n\\nStudent (Scratch) Run: Initial\")\n",
    "train_and_evaluate(student_scratch_model, compute_student_scratch_loss, NUM_INIT_EPOCHS, 1e-3, temperature=4)\n",
    "print(\"\\nStudent (Scratch) Run: Fine\")\n",
    "ss_acc = train_and_evaluate(student_scratch_model, compute_student_scratch_loss, NUM_FINE_EPOCHS, 1e-4, temperature=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj1N38fnRTNB"
   },
   "source": [
    "# Test Accuracy vs. Temperature Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gX4dbazrRWIz"
   },
   "outputs": [],
   "source": [
    "# your code start from here for step 6\n",
    "\n",
    "# Original student uses temperature 4\n",
    "student_kd_model1 = tf.keras.Sequential()\n",
    "student_kd_model1.add(tf.keras.applications.mobilenet_v2.MobileNetV2(classifier_activation = None, input_shape = (224, 224, 3)))\n",
    "student_kd_model1.add(tf.keras.layers.Dense(NUM_CLASSES)) \n",
    "\n",
    "student_kd_model2 = tf.keras.Sequential()\n",
    "student_kd_model2.add(tf.keras.applications.mobilenet_v2.MobileNetV2(classifier_activation = None, input_shape = (224, 224, 3)))\n",
    "student_kd_model2.add(tf.keras.layers.Dense(NUM_CLASSES)) \n",
    "\n",
    "student_kd_model16 = tf.keras.Sequential()\n",
    "student_kd_model16.add(tf.keras.applications.mobilenet_v2.MobileNetV2(classifier_activation = None, input_shape = (224, 224, 3)))\n",
    "student_kd_model16.add(tf.keras.layers.Dense(NUM_CLASSES)) \n",
    "\n",
    "student_kd_model32 = tf.keras.Sequential()\n",
    "student_kd_model32.add(tf.keras.applications.mobilenet_v2.MobileNetV2(classifier_activation = None, input_shape = (224, 224, 3)))\n",
    "student_kd_model32.add(tf.keras.layers.Dense(NUM_CLASSES)) \n",
    "\n",
    "student_kd_model64 = tf.keras.Sequential()\n",
    "student_kd_model64.add(tf.keras.applications.mobilenet_v2.MobileNetV2(classifier_activation = None, input_shape = (224, 224, 3)))\n",
    "student_kd_model64.add(tf.keras.layers.Dense(NUM_CLASSES)) \n",
    "\n",
    "student_dict = {1: student_kd_model1, 2: student_kd_model2, 4: student_kd_model, 16: student_kd_model16, 32: student_kd_model32, 64: student_kd_model64}\n",
    "acc_dict = {4: skd_acc}\n",
    "\n",
    "for temp in [1, 2, 16, 32, 64]:\n",
    "    print(f'Student (KD, Temperature = {temp}) Run: Initial')\n",
    "    train_and_evaluate(student_dict[temp], compute_student_loss, NUM_INIT_EPOCHS, 1e-3, teacher_model=teacher_model, temperature=temp, alpha=0.5)\n",
    "    print(f'\\nStudent (KD, Temperature = {temp}) Run: Fine')\n",
    "    acc_dict[temp] = train_and_evaluate(student_dict[temp], compute_student_loss, NUM_FINE_EPOCHS, 1e-4, teacher_model=teacher_model, temperature=temp, alpha=0.5)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNrH_1emRbGA"
   },
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "teacher_model.save('teacher.h5')\n",
    "student_kd_model.save('student_kd4.h5')\n",
    "student_scratch_model.save('student_scratch.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq3JTpQ4RuhR"
   },
   "source": [
    "# Comparing the teacher and student model (number of of parameters and FLOPs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4V8GB2yRRuxF"
   },
   "outputs": [],
   "source": [
    "# your code start from here for step 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjwJ5oziRvRn"
   },
   "source": [
    "# Implementing the state-of-the-art KD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q10lybAFRvZt"
   },
   "outputs": [],
   "source": [
    "# your code start from here for step 13\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Task1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
